---
layout: post
title:  "逻辑回归原理及其python实现"
categories: 大数据
tags: 逻辑回归 正则化 梯度下降
author: y570pc
---

* content
{:toc}

## 原理

#### 逻辑回归模型：

*注：github不支持mathjax的公式渲染，需要安装chrome的GitHub with MathJax插件

$h_{\theta}(x)=\frac{1}{1+e^{-{\theta}^{T}x}}$

#### 逻辑回归代价函数：
$J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}(x^{(i)}),y^{(i)})$

其中：

$$ 
Cost(h_{\theta}(x),y)=
\begin{cases}
-log(h_{\theta}(x))& \text{y=1}\\
-log(1-h_{\theta}(x))& \text{y=0}
\end{cases}
$$

该式子合并后：

$Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))$

即逻辑回归的代价函数：

$J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$ 

#### 最小化代价函数，使用梯度下降法（gradient descent）。

**Want** $min_{\theta}J(\theta):$

**Repeat** {

$\theta_{j}=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta)$

}&emsp;&emsp;(simultaneously updata all $\theta_{j}$，$\alpha$为学习率)

即:

**Repeat** {

$\theta_{j}=\theta_{j}-\alpha\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$

}

#### 正则化（Regularization）

如果我们有非常多的特征,我们通过学习得到的模型可能能够非常好地适应训练集,但是可能不能推广到新的数据集，我们把这种现象成为**过拟合**。

![1](/img/2018-09-28-01.png)

为防止过拟合，提升模型泛化能力，我们需要对所有特征参数（除$\theta_{0}$外）进行惩罚，即保留所有特征，减小参数$\theta$的值，当我们拥有很多不太有用ed特征时，正则化会起到很好的作用。

$J(\theta)=-[\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}log(h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-h_{\theta}(x^{(i)})))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_{j}^{2}$

梯度下降算法：

**Repeat** until convergence{

$\theta_{0}=\theta_{0}-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x^{(i)})-y^{(i)})\cdot x_{0}^{(i)})$

$\theta_{j}=\theta_{j}-\alpha\frac{1}{m}\sum_{i=1}^{m}((h_{\theta}(x^{(i)})-y^{(i)})\cdot x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j})$ &emsp;&emsp; $for\hspace{1em}j=1,2,...n$

}   








